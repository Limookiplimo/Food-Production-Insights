{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf38101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages and libraries\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "\n",
    "#filter out warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d4d6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F-A-O\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b44b7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maxToStringFields property\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a6281e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "prod_data = spark\\\n",
    "            .read\\\n",
    "            .options(inferSchema=\"true\", header=\"true\")\\\n",
    "            .csv(\"Production_Crops_Livestock_E_Africa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bebc0b",
   "metadata": {},
   "source": [
    "There are columns that do not add direct context to my analysis. I will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5a6e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "\n",
    "# drop flag cols\n",
    "to_drop = [col for col in prod_data.columns if col.endswith(\"F\")]\n",
    "\n",
    "# drop unnecessary cols\n",
    "to_drop1 = ['Area Code','Area Code (M49)','Item Code','Item Code (CPC)','Element Code','Unit']\n",
    "\n",
    "# implement\n",
    "prod_data = (prod_data\\\n",
    "            .drop(*to_drop, *to_drop1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e218f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "#rename year cols\n",
    "renamed_cols = [col.replace('Y', '') for col in prod_data.columns]\n",
    "prod_data = prod_data.toDF(*renamed_cols)\n",
    "\n",
    "#rename other cols\n",
    "prod_data = prod_data\\\n",
    "                .withColumnRenamed(\"Area\", \"Country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b97b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Element: string (nullable = true)\n",
      " |-- 1961: double (nullable = true)\n",
      " |-- 1962: double (nullable = true)\n",
      " |-- 1963: double (nullable = true)\n",
      " |-- 1964: double (nullable = true)\n",
      " |-- 1965: double (nullable = true)\n",
      " |-- 1966: double (nullable = true)\n",
      " |-- 1967: double (nullable = true)\n",
      " |-- 1968: double (nullable = true)\n",
      " |-- 1969: double (nullable = true)\n",
      " |-- 1970: double (nullable = true)\n",
      " |-- 1971: double (nullable = true)\n",
      " |-- 1972: double (nullable = true)\n",
      " |-- 1973: double (nullable = true)\n",
      " |-- 1974: double (nullable = true)\n",
      " |-- 1975: double (nullable = true)\n",
      " |-- 1976: double (nullable = true)\n",
      " |-- 1977: double (nullable = true)\n",
      " |-- 1978: double (nullable = true)\n",
      " |-- 1979: double (nullable = true)\n",
      " |-- 1980: double (nullable = true)\n",
      " |-- 1981: double (nullable = true)\n",
      " |-- 1982: double (nullable = true)\n",
      " |-- 1983: double (nullable = true)\n",
      " |-- 1984: double (nullable = true)\n",
      " |-- 1985: double (nullable = true)\n",
      " |-- 1986: double (nullable = true)\n",
      " |-- 1987: double (nullable = true)\n",
      " |-- 1988: double (nullable = true)\n",
      " |-- 1989: double (nullable = true)\n",
      " |-- 1990: double (nullable = true)\n",
      " |-- 1991: double (nullable = true)\n",
      " |-- 1992: double (nullable = true)\n",
      " |-- 1993: double (nullable = true)\n",
      " |-- 1994: double (nullable = true)\n",
      " |-- 1995: double (nullable = true)\n",
      " |-- 1996: double (nullable = true)\n",
      " |-- 1997: double (nullable = true)\n",
      " |-- 1998: double (nullable = true)\n",
      " |-- 1999: double (nullable = true)\n",
      " |-- 2000: double (nullable = true)\n",
      " |-- 2001: double (nullable = true)\n",
      " |-- 2002: double (nullable = true)\n",
      " |-- 2003: double (nullable = true)\n",
      " |-- 2004: double (nullable = true)\n",
      " |-- 2005: double (nullable = true)\n",
      " |-- 2006: double (nullable = true)\n",
      " |-- 2007: double (nullable = true)\n",
      " |-- 2008: double (nullable = true)\n",
      " |-- 2009: double (nullable = true)\n",
      " |-- 2010: double (nullable = true)\n",
      " |-- 2011: double (nullable = true)\n",
      " |-- 2012: double (nullable = true)\n",
      " |-- 2013: double (nullable = true)\n",
      " |-- 2014: double (nullable = true)\n",
      " |-- 2015: double (nullable = true)\n",
      " |-- 2016: double (nullable = true)\n",
      " |-- 2017: double (nullable = true)\n",
      " |-- 2018: double (nullable = true)\n",
      " |-- 2019: double (nullable = true)\n",
      " |-- 2020: double (nullable = true)\n",
      " |-- 2021: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim column names\n",
    "prod_data = prod_data.select([col(name).alias(name.strip()) for name in prod_data.columns])\n",
    "prod_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0746f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt dataframe\n",
    "#convert spark to pandas df\n",
    "prod_data = ps.DataFrame(prod_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0600bd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Item</th>\n",
       "      <th>Element</th>\n",
       "      <th>Year</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>Area harvested</td>\n",
       "      <td>1961</td>\n",
       "      <td>13300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>Area harvested</td>\n",
       "      <td>1962</td>\n",
       "      <td>13300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>Area harvested</td>\n",
       "      <td>1963</td>\n",
       "      <td>13300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>Area harvested</td>\n",
       "      <td>1964</td>\n",
       "      <td>14200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>Area harvested</td>\n",
       "      <td>1965</td>\n",
       "      <td>13800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country               Item         Element  Year   Weight\n",
       "0  Algeria  Almonds, in shell  Area harvested  1961  13300.0\n",
       "1  Algeria  Almonds, in shell  Area harvested  1962  13300.0\n",
       "2  Algeria  Almonds, in shell  Area harvested  1963  13300.0\n",
       "3  Algeria  Almonds, in shell  Area harvested  1964  14200.0\n",
       "4  Algeria  Almonds, in shell  Area harvested  1965  13800.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#melt the pandas df\n",
    "keep_columns=['Country', 'Item', 'Element']\n",
    "prod_data=prod_data.melt(id_vars=keep_columns, var_name='Year',value_name='Weight')\n",
    "prod_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d84583",
   "metadata": {},
   "source": [
    "I converted the dataframe to pandas dataframe in order to melt it. I could not get along with unpivoting the dataframe with pyspark. If you know how, kindly reach out.\n",
    "Now I'll have to covert the dataframe back to spark DF for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3aa51110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe to RDD \n",
    "rdd = spark.sparkContext.parallelize(prod_data.values.tolist())\n",
    "\n",
    "#Schema for the pyspark dataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Country\", StringType(), nullable=False),\n",
    "    StructField(\"Item\", StringType(), nullable=False),\n",
    "    StructField(\"Element\", StringType(), nullable=False),\n",
    "    StructField(\"Year\", StringType(), nullable=False),\n",
    "    StructField(\"Weight\", DoubleType(), nullable=False)\n",
    "])\n",
    "\n",
    "#create pyspark dataFrame\n",
    "prod_data = spark.createDataFrame(rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13af194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+----+-------+--------+\n",
      "|Country|             Item|       Element|Year| Weight|Category|\n",
      "+-------+-----------------+--------------+----+-------+--------+\n",
      "|Algeria|Almonds, in shell|Area harvested|1961|13300.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1962|13300.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1963|13300.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1964|14200.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1965|13800.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1966|12700.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1967|12900.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1968|10000.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1969| 5500.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1970| 6700.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1971| 4600.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1972| 8300.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1973| 9200.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1974|10150.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1975|11000.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1976|18250.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1977|10400.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1978| 7100.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1979| 8100.0|        |\n",
      "|Algeria|Almonds, in shell|Area harvested|1980| 8700.0|        |\n",
      "+-------+-----------------+--------------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:29:28 WARN TaskSetManager: Stage 39 contains a task of very large size (3721 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "prod_data = prod_data.withColumn(\"Category\",lit(\"\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "prod_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68131ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48118452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907ccb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86282833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6434bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6a428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22babee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5aacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meditation HQ\n",
    "#======================================\n",
    "# .withColumnRenamed(\"year\", trim(col(\"year\")).alias(\"year\"))\n",
    "# keep_columns = ['Country', 'Item', 'Element']\n",
    "# other_columns = [col(column) for column in prod_data.columns if column not in keep_columns]\n",
    "# other_df = prod_data.select(*other_columns)\n",
    "# other_df.printSchema()\n",
    "\n",
    "#===============================\n",
    "# # Assuming you have a DataFrame named 'df' with the year columns and other columns\n",
    "# keep_columns = ['Area', 'Item', 'Element']\n",
    "# year_columns = [col for col in prod_data.columns if col not in keep_columns]\n",
    "\n",
    "#==========================\n",
    "# # Melt the year columns\n",
    "# prod_data = prod_data.select(keep_columns + [\n",
    "#     explode(\n",
    "#         [\n",
    "#             (lit(col).alias('year'), col)\n",
    "#             for col in year_columns\n",
    "#         ]\n",
    "#     ).alias('melted')\n",
    "# ]).select(keep_columns + [\n",
    "#     col('melted.year'),\n",
    "#     col('melted.weight')\n",
    "# ])\n",
    "# prod_data.show()\n",
    "\n",
    "#=========================\n",
    "# melted_df = renamed_df.selectExpr(\"stack(61, \" + \", \".join([f\"'{col}', {col}\" for col in renamed_df.columns]) + \") as (year, weight)\")\n",
    "\n",
    "#====================\n",
    "# from pyspark.sql.functions import expr\n",
    "# melted_df = other_df.selectExpr(\"posexplode(array(*)) as (year, weight)\").select(\"year\", \"weight\")\n",
    "# melted_df.show()\n",
    "\n",
    "#===================\n",
    "# cols = other_df.columns\n",
    "# other_df = other_df.selectExpr(\"stack({},{})\".format(len(cols), ','.join((\"'{}'\".format(i) for i in cols))))\n",
    "# other_df.show()\n",
    "\n",
    "#===================\n",
    "# # Configure output partitions\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "#=================\n",
    "# #partitionSizes = prod_data.rdd.glom().map(len).collect()\n",
    "# print(\"Size of partitions:\", partitionSizes)\n",
    "\n",
    "#==============\n",
    "# numPartitions = prod_data.rdd.getNumPartitions()\n",
    "# print(\"Number of partitions:\", numPartitions)\n",
    "\n",
    "#===================\n",
    "# repartitionedDF = prod_data.repartition(32)\n",
    "\n",
    "#==============\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# executor_memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "# print(\"Executor Memory:\", executor_memory)\n",
    "\n",
    "#=================\n",
    "#prod_data.explain(\"formatted\")\n",
    "\n",
    "#=========================\n",
    "# Set the maxPartitionBytes property\n",
    "# spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"256m\")\n",
    "\n",
    "#====================\n",
    "# # Get unique items in the \"Name\" column\n",
    "# unique_names = prod_data.select(\"Item\").distinct()\n",
    "# # Show the unique names\n",
    "# unique_names.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfef26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
