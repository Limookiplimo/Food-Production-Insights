{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf38101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limoo/anaconda3/envs/spark/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Required packages and libraries\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "\n",
    "#filter out warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4d6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 16:11:20 WARN Utils: Your hostname, drice resolves to a loopback address: 127.0.1.1; using 192.168.51.125 instead (on interface enp44s0)\n",
      "23/06/05 16:11:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/05 16:11:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initalize spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F-A-O\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44b7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maxToStringFields property\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6281e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "prod_data = spark\\\n",
    "            .read\\\n",
    "            .options(inferSchema=\"true\", header=\"true\")\\\n",
    "            .csv(\"Production_Crops_Livestock_E_Africa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bebc0b",
   "metadata": {},
   "source": [
    "There are columns that do not add direct context to my analysis. I will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a6e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "\n",
    "# drop flag cols\n",
    "to_drop = [col for col in prod_data.columns if col.endswith(\"F\")]\n",
    "\n",
    "# drop unnecessary cols\n",
    "to_drop1 = ['Area Code','Area Code (M49)','Item Code','Item Code (CPC)','Element','Element Code','Unit']\n",
    "\n",
    "# implement\n",
    "prod_data = (prod_data\\\n",
    "            .drop(*to_drop, *to_drop1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e218f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "#rename year cols\n",
    "renamed_cols = [col.replace('Y', '') for col in prod_data.columns]\n",
    "prod_data = prod_data.toDF(*renamed_cols)\n",
    "\n",
    "#rename other cols\n",
    "prod_data = prod_data\\\n",
    "                .withColumnRenamed(\"Area\", \"Country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b97b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim column names\n",
    "prod_data = prod_data.select([col(name).alias(name.strip()) for name in prod_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0746f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt dataframe\n",
    "#convert spark to pandas df\n",
    "prod_data = ps.DataFrame(prod_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0600bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>1961</td>\n",
       "      <td>13300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>1962</td>\n",
       "      <td>13300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>1963</td>\n",
       "      <td>13300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>1964</td>\n",
       "      <td>14200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Almonds, in shell</td>\n",
       "      <td>1965</td>\n",
       "      <td>13800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country               Item  Year   Weight\n",
       "0  Algeria  Almonds, in shell  1961  13300.0\n",
       "1  Algeria  Almonds, in shell  1962  13300.0\n",
       "2  Algeria  Almonds, in shell  1963  13300.0\n",
       "3  Algeria  Almonds, in shell  1964  14200.0\n",
       "4  Algeria  Almonds, in shell  1965  13800.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#melt the pandas df\n",
    "keep_columns=['Country', 'Item']\n",
    "prod_data=prod_data.melt(id_vars=keep_columns, var_name='Year',value_name='Weight')\n",
    "prod_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d84583",
   "metadata": {},
   "source": [
    "I converted the dataframe to pandas dataframe in order to melt it. I could not get along with unpivoting the dataframe with pyspark. If you know how, kindly reach out.\n",
    "Now I'll have to covert the dataframe back to spark DF for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa51110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe to RDD \n",
    "r_dd = spark.sparkContext.parallelize(prod_data.values.tolist())\n",
    "\n",
    "#Schema for the pyspark dataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Country\", StringType(), nullable=False),\n",
    "    StructField(\"Item\", StringType(), nullable=False),\n",
    "    StructField(\"Year\", StringType(), nullable=False),\n",
    "    StructField(\"Weight\", DoubleType(), nullable=False)\n",
    "])\n",
    "\n",
    "#create pyspark dataFrame\n",
    "prod_data = spark.createDataFrame(r_dd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13af194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column\n",
    "prod_data = prod_data.withColumn(\"Category\",lit(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48118452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize items\n",
    "grains=['Wheat and products', 'Rice (Milled Equivalent)','Barley and products','Maize and products',\n",
    "        'Millet and products','Cereals, Other','Cereals - Excluding Beer','Rye and products','Oats',\n",
    "        'Sorghum and products','Cassava and products','Cereals n.e.c.','Wheat','Maize (corn)','Sorghum','Triticale',\n",
    "       'Fonio']\n",
    "vegetables=['Potatoes and products','Vegetables, Other','Starchy Roots','Vegetables','Sweet potatoes','Roots, Other',\n",
    "            'Onions','Plantains','Pimento','Aquatic Plants','Yams','Potatoes','Roots and Tubers, Total',\n",
    "           'Eggplants (aubergines)','Cauliflowers and broccoli','Edible roots and tubers with high starch or inulin content, n.e.c., fresh',\n",
    "           'Mushrooms and truffles','Other vegetables, fresh n.e.c.','Apricots','Roots and Tubers, Total',\n",
    "           'Tomatoes','Cucumbers and gherkins','Chillies and peppers, green (Capsicum spp. and Pimenta spp.)',\n",
    "           'Cabbages','Carrots and turnips','Cassava, fresh']\n",
    "sugars=['Sugar (Raw Equivalent)','Sweeteners, Other','Sugar Crops','Sugar & Sweeteners','Sugar non-centrifugal',\n",
    "        'Sugar beet','Sugar cane','Molasses','Raw cane or beet sugar (centrifugal only)']\n",
    "fruits=['Olives-Including Preserved','Tomatoes and products','Oranges, Mandarines','Citrus, Other','Bananas',\n",
    "        'Apples and products','Pineapples and products','Dates','Grapes and Products-Excluding Wine','Fruits, Other',\n",
    "        'Fruits - Excluding Wine','Lemons, Limes and products','Grapefruit and products','Pineapples',\n",
    "        'Plums and sloes','Watermelons','Figs','Pomelos and grapefruits','Other fruits, n.e.c.','Fruit Primary',\n",
    "       'Tangerines, mandarins, clementines','Mangoes, guavas and mangosteens','Quinces','Grapes']\n",
    "legumes=['Pulses, Other and products','Coffee and products','Cocoa Beans and products','Pulses','Beans','Peas',\n",
    "         'Soyabeans','Groundnuts-Shelled Eq','Cocoa beans','Broad beans and horse beans, green','Other beans, green',\n",
    "        '|Lentils, dry','Okra','Cashew nuts, in shell','Groundnuts, excluding shelled','Vetches','Soya beans',\n",
    "        'Broad beans and horse beans, dry']\n",
    "seeds=['Sesame seed', 'Rape and Mustardseed','Palm kernels','Sunflower seed','Cottonseed','Castor oil seeds',\n",
    "      'Rape or colza seed','Melonseed','Cotton seed']\n",
    "nuts=['Nuts and products','Almonds, in shell','Coconuts - Incl Copra','Treenuts','Coconuts, in shell']\n",
    "oils_fats=['Soyabean Oil','Groundnut Oil','Sunflowerseed Oil','Rape and Mustard Oil','Cottonseed Oil','Palm Oil',\n",
    "           'Sesameseed Oil','Olive Oil','Oilcrops Oil, Other','Oilcrops','Vegetable Oils','Oilcrops, Other',\n",
    "           'Maize Germ Oil','Coconut Oil','Palmkernel Oil','Ricebran Oil','Fish, Body Oil','Fish, Liver Oil',\n",
    "           'Animal fats','Fats, Animals, Raw','Olive oil','Rapeseed or canola oil, crude','Cheese (All Kinds)',\n",
    "           'Olives','Oil of palm kernel','Palm oil','Cattle fat, unrendered','Groundnut oil',\n",
    "          'Safflower-seed oil, crude','Pig fat, rendered','Goat fat, unrendered','Margarine and shortening']\n",
    "beverages=['Tea-Including Mate','Wine','Beer','Beverages, Alcoholic','Alcoholic Beverages','Beverages, Fermented',\n",
    "           'Coffee, green']\n",
    "spices=['Pepper','Spices, Other','Spices','Cloves','Green garlic']\n",
    "meat=['Bovine Meat','Mutton & Goat Meat','Meat, Other','Meat','Pigmeat','Game meat, fresh, chilled or frozen',\n",
    "     'Horse meat, fresh or chilled']\n",
    "sea_food=['Freshwater Fish','Fish, Seafood','Demersal Fish','Pelagic Fish','Marine Fish, Other','Crustaceans',\n",
    "          'Cephalopods','Molluscs, Other','Aquatic Animals, Others','Aquatic Products, Other',\n",
    "          'Meat, Aquatic Mammals']\n",
    "dairy=['Butter, Ghee','Cream','Milk - Excluding Butter','Infant food','Butter of cow milk','Skim Milk & Buttermilk, Dry',\n",
    "       'Buttermilk, dry','Cheese from skimmed cow milk','Skim milk of cows','Cheese from whole cow milk',\n",
    "       'Milk, Total','Raw milk of sheep']\n",
    "poultry=['Eggs','Poultry Meat','Turkeys','Chickens','Hen eggs in shell, fresh','Poultry Birds']\n",
    "other_animal_products=['Honey','Bees','Offals','Offals, Edible','Edible offals of horses and other equines,  fresh, chilled or frozen',\n",
    "                      'Raw hides and skins of sheep or lambs','Other meat n.e.c. (excluding mammals), fresh, chilled or frozen',\n",
    "                      'Edible offal of cattle, fresh, chilled or frozen','Sheep and Goats','Asses','Raw hides and skins of cattle',\n",
    "                      'Beeswax','Edible offals of camels and other camelids, fresh, chilled or frozen',\n",
    "                      'Rabbits and hares','Raw hides and skins of goats or kids','Camels','Swine / pigs']\n",
    "miscellaneous=['Stimulants','Miscellaneous']\n",
    "other_plant_products=['Flax, processed but not spun','Unmanufactured tobacco','Sisal, raw','Cotton lint, ginned','Fibre Crops, Fibre Equivalent']\n",
    "\n",
    "# Pattern matching with regex\n",
    "grains_regex = '|'.join(grains)\n",
    "vegetables_regex = '|'.join(vegetables)\n",
    "sugars_regex= '|'.join(sugars)\n",
    "fruits_regex = '|'.join(fruits)\n",
    "legumes_regex = '|'.join(legumes)\n",
    "seeds_regex = '|'.join(seeds)\n",
    "nuts_regex = '|'.join(nuts)\n",
    "oils_fats_regex = '|'.join(oils_fats)\n",
    "beverages_regex = '|'.join(beverages)\n",
    "spices_regex = '|'.join(spices)\n",
    "meat_regex = '|'.join(meat)\n",
    "sea_food_regex = '|'.join(sea_food)\n",
    "dairy_regex = '|'.join(dairy)\n",
    "poultry_regex = '|'.join(poultry)\n",
    "other_animal_products_regex = '|'.join(other_animal_products)\n",
    "miscellaneous_regex = '|'.join(miscellaneous)\n",
    "other_plant_products_regex = '|'.join(other_plant_products)\n",
    "\n",
    "# Update \"Category\" column based on the values in \"Item\" column\n",
    "prod_data = prod_data.withColumn(\"Category\",\n",
    "                                 when(col(\"Item\").rlike(grains_regex), lit(\"Grains\"))\n",
    "                                 .when(col(\"Item\").rlike(vegetables_regex), lit(\"Vegetables\"))\n",
    "                                 .when(col(\"Item\").rlike(sugars_regex), lit(\"Sugar\"))\n",
    "                                 .when(col(\"Item\").rlike(fruits_regex), lit(\"Fruits\"))\n",
    "                                 .when(col(\"Item\").rlike(legumes_regex), lit(\"Legumes\"))\n",
    "                                 .when(col(\"Item\").rlike(seeds_regex), lit(\"Seeds\"))\n",
    "                                 .when(col(\"Item\").rlike(nuts_regex), lit(\"Nuts\"))\n",
    "                                 .when(col(\"Item\").rlike(oils_fats_regex), lit(\"Oil_Fats\"))\n",
    "                                 .when(col(\"Item\").rlike(beverages_regex), lit(\"Beverages\"))\n",
    "                                 .when(col(\"Item\").rlike(spices_regex), lit(\"Spices\"))\n",
    "                                 .when(col(\"Item\").rlike(meat_regex), lit(\"Meat\"))\n",
    "                                 .when(col(\"Item\").rlike(sea_food_regex), lit(\"Sea_Food\"))\n",
    "                                 .when(col(\"Item\").rlike(dairy_regex), lit(\"Dairy\"))\n",
    "                                 .when(col(\"Item\").rlike(poultry_regex), lit(\"Poultry\"))\n",
    "                                 .when(col(\"Item\").rlike(other_animal_products_regex), lit(\"Other_Animal_Products\"))\n",
    "                                 .when(col(\"Item\").rlike(miscellaneous_regex), lit(\"Miscellaneous\"))\n",
    "                                 .when(col(\"Item\").rlike(other_plant_products_regex), lit(\"Other_Plant_Products\"))\n",
    "                                 .otherwise(col(\"Category\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e907ccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 16:44:11 WARN TaskSetManager: Stage 6 contains a task of very large size (2928 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 6:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Item|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_counts = prod_data.filter(col(\"Category\") == \"\").groupBy(\"Item\").count()\n",
    "item_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86282833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6434bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6a428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22babee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5aacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meditation HQ\n",
    "#======================================\n",
    "# .withColumnRenamed(\"year\", trim(col(\"year\")).alias(\"year\"))\n",
    "# keep_columns = ['Country', 'Item', 'Element']\n",
    "# other_columns = [col(column) for column in prod_data.columns if column not in keep_columns]\n",
    "# other_df = prod_data.select(*other_columns)\n",
    "# other_df.printSchema()\n",
    "\n",
    "#===============================\n",
    "# # Assuming you have a DataFrame named 'df' with the year columns and other columns\n",
    "# keep_columns = ['Area', 'Item', 'Element']\n",
    "# year_columns = [col for col in prod_data.columns if col not in keep_columns]\n",
    "\n",
    "#==========================\n",
    "# # Melt the year columns\n",
    "# prod_data = prod_data.select(keep_columns + [\n",
    "#     explode(\n",
    "#         [\n",
    "#             (lit(col).alias('year'), col)\n",
    "#             for col in year_columns\n",
    "#         ]\n",
    "#     ).alias('melted')\n",
    "# ]).select(keep_columns + [\n",
    "#     col('melted.year'),\n",
    "#     col('melted.weight')\n",
    "# ])\n",
    "# prod_data.show()\n",
    "\n",
    "#=========================\n",
    "# melted_df = renamed_df.selectExpr(\"stack(61, \" + \", \".join([f\"'{col}', {col}\" for col in renamed_df.columns]) + \") as (year, weight)\")\n",
    "\n",
    "#====================\n",
    "# from pyspark.sql.functions import expr\n",
    "# melted_df = other_df.selectExpr(\"posexplode(array(*)) as (year, weight)\").select(\"year\", \"weight\")\n",
    "# melted_df.show()\n",
    "\n",
    "#===================\n",
    "# cols = other_df.columns\n",
    "# other_df = other_df.selectExpr(\"stack({},{})\".format(len(cols), ','.join((\"'{}'\".format(i) for i in cols))))\n",
    "# other_df.show()\n",
    "\n",
    "#===================\n",
    "# # Configure output partitions\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "#=================\n",
    "# #partitionSizes = prod_data.rdd.glom().map(len).collect()\n",
    "# print(\"Size of partitions:\", partitionSizes)\n",
    "\n",
    "#==============\n",
    "# numPartitions = prod_data.rdd.getNumPartitions()\n",
    "# print(\"Number of partitions:\", numPartitions)\n",
    "\n",
    "#===================\n",
    "# repartitionedDF = prod_data.repartition(32)\n",
    "\n",
    "#==============\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# executor_memory = sc.getConf().get(\"spark.executor.memory\")\n",
    "# print(\"Executor Memory:\", executor_memory)\n",
    "\n",
    "#=================\n",
    "#prod_data.explain(\"formatted\")\n",
    "\n",
    "#=========================\n",
    "# Set the maxPartitionBytes property\n",
    "# spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"256m\")\n",
    "\n",
    "#====================\n",
    "# # Get unique items in the \"Name\" column\n",
    "# unique_names = prod_data.select(\"Item\").distinct()\n",
    "# # Show the unique names\n",
    "# unique_names.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfef26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
